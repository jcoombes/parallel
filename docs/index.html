<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Asynchronous Programming in Python and Rust - PyCon DE & PyData 2025</title>
    
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@400;500;600&display=swap" rel="stylesheet">
    
    <!-- Reveal.js CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/dist/reset.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/dist/theme/black.css" id="theme">
    
    <!-- Highlight.js and theme -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.8.0/styles/vs2015.css">
    
    <!-- Custom styles -->
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- SECTION 1: INTRODUCTION -->
            <section>
                <h1>ðŸ¦€ RÃ¼stzeit</h1>
                <h3>Asynchronous and Concurrent Optimisation in Python and Rust</h3>
                <p>Jamie Coombes</p>
                <p class="conference-info">PyCon DE & PyData 2025</p>
                <p class="conference-track">PyCon: Rust</p>
                <p class="conference-date">23. - 25. April 2025</p>
                <p class="location">Darmstadtium, Darmstadt, Deutschland</p>
            </section>

            <!-- Talk Overview -->
            <section>
                <ol>
                    <li><strong>Main Talk Question</strong>
                        <p>Python/Rust Interop and David Hewitt's free-threaded python atop Tokio question.</p>
                    </li>
                    <li><strong>Asynchronous Concurrency</strong>
                        <p>Exploring async foundations through Rayon and Tokio in Rust, compared with Python's approaches.</p>
                    </li>
                    <li><strong>Case Study: A million monkeys at a million typewriters</strong>
                        <p>Parallelising and benchmarking a genetic algorithm for evolving text into Shakespeare quotes.</p>
                    </li>
                    <li><strong>Conclusion</strong>
                        <p>Exploring the future of Python/Rust Interop and the potential for free-threaded Python atop Tokio.</p>
                    </li>
                </ol>
            </section>

            <!-- Test Mermaid Section with vertical slides -->
            <section>
                <!-- Top slide with basic diagram -->
                <section>
                    <h2>Test Mermaid Diagram</h2>
                    <div class="mermaid">
                        flowchart TD
                            A[Start] --> B{Decision}
                            B -->|Yes| C[Do Something]
                            B -->|No| D[Do Nothing]
                            C --> E[End]
                            D --> E
                    </div>
                    <p><small>â†“ Press down for more examples</small></p>
                </section>

                <!-- Example image slide -->
                <section>
                    <h2>Example Image</h2>
                    <img src="images/flame-graph.png" alt="Example Flame Graph" style="height: 400px;">
                    <p><small>Example performance flame graph visualization</small></p>
                </section>

                <!-- Example complex diagram slide -->
                <section>
                    <h2>Example Complex Diagram</h2>
                    <div class="mermaid">
                        flowchart LR
                            A[Python Code] -->|PyO3| B{Rust Bridge}
                            B -->|Async| C[Tokio Runtime]
                            B -->|Sync| D[Native Thread]
                            C -->|Events| E[IO Operations]
                            D -->|Compute| F[CPU Tasks]
                            E --> B
                            F --> B
                    </div>
                    <p><small>Python-Rust interop architecture diagram</small></p>
                </section>
            </section>

            <!-- Rust/Python Interop Growth -->
            <section>
                <section>
                    <h2>Python/Rust Interop Growth in 2025</h2>
                    <img src="images/pypi-rust-growth.png" alt="Growth of PyPI packages with Rust components.">
                </section>
                <section>
                    <ul>
                        <h2>Rust/Python Interop Growth in 2025</h2>
                        <li>Rust language as preferred developer-experience for writing native python extensions.</li>
                        <li>RIIR - Rewrite It In Rust - for 10x-80x* performance improvements</li>
                        <li>PyO3 is the most popular Rust library for Python interop.</li>
                        <li>Maturin is the most popular tool for building and distributing Rust Python packages.</li>
                        <small>*performance benchmarking is hard, see flame graphs later</small>
                    </ul>
                </section>
            </section>
    

            <!-- Inciting Question -->
            <section>
                <section>
                    <h2>The Main Talk Questions</h2>
                    <blockquote>
                        "Do we want to expose the tokio event loop as the basis for a Python async runtime?"
                    </blockquote>
                    <p>â€” David Hewitt <a href="https://x.com/davidhewittdev">@davidhewittdev</a> - Rust Nation UK 2025</p>
                    <blockquote>
                        "How will free-threaded Python lead to better parallel code?"
                    </blockquote>
                    <p>- me, today, right now</p>
                </section>

                <!-- Async Runtime Diagram -->
                <section>
                    <h2>Free-threaded Python + Tokio = ??</h2>
                    <img src="assets/tokio-based-python-runtime-dark.svg" alt="Comparison of single-threaded asyncio vs multi-threaded tokio-based Python runtime">
                </section>
            </section>

            <section>
                <h2>Free Threading in Python</h2>
                <p>Python 3.13 shipped an <italic>experimental</italic> variant without the "Global Interpreter Lock"
                <ul>
                    <li>Expected to become the default in a few years</li>
                    <li>PyO3 supports 3.13t fully</li>
                    <li>Rust's Send + Sync traits make it straightforward to reason about threading issues</li>
                </ul>
            </section>

            <!-- Tokio-based Python Runtime benefits-->
            <section>
                <section>
                    <h2>A multi-threaded tokio-based Python runtime could be...</h2>
                </section>
                <section>
                    <h2>Memory Efficient (and hence scalable)</h2>
                        <img src="images/tokio-memory-usage-10k.png" alt="Comparison of memory usage between tokio and other async runtimes">
                        <p>Image source: <a href="https://pkolaczk.github.io/memory-consumption-of-async/">Pkolaczk's memory-consumption-of-async</a></p>
                </section>
                <section>
                    <h2>...well past os.cpu_count() threads</h2>
                    <div class="chart-placeholder">
                        <img src="images/tokio-memory-usage-1M.png" alt="Comparison of memory usage between tokio and other async runtimes">
                        <p>Image source: <a href="https://pkolaczk.github.io/memory-consumption-of-async/">Pkolaczk's memory-consumption-of-async</a></p>
                    </div>
                </section>
                <section>
                    <h2>Typed to prevent data races</h2>
                    <ul>
                        <li>Rust's ownership model guarantees thread safety at compile time</li>
                        <li>Rich ecosystem of threading primitives (std::sync, parking_lot, crossbeam)</li>
                        <li>Threading can complement or replace async patterns</li>
                    </ul>
                    <p><small>Note: While Rust prevents data races, it can't prevent all concurrency bugs (deadlocks, algorithmic issues)</small></p>
                </section>
                <section>
                    <h2>Faster</h2>
                    <p>so you add :rocket: to your project README</p>
                </section>
            </section>

            <section>
                <section>
                    <h2>okay, but I really like python</h2>
                    <p>what are the multi-threaded async alternatives?</p>
                    <p>well there is threado...</p>
                </section>
                <section>
                    <h2>but threado is deado</h2>
                    <div class="chart-placeholder">
                        <img src="images/threado-dead.png" alt="threado is super dead">
                    </div>
                    <p>maybe ask <a href="@dabeaz@mastodon.social">@dabeaz</a> to bring it back for 3.14t or 3.15t?</p>
                    <p>or just do dabeaz's <a href="https://www.dabeaz.com/summer.html">summer of rust</a> and create a tokio-based python runtime?</p>
                </section>
            </section>

            <section>
                <h2>How do rust and python think about concurrency and parallelism?</h2>
            </section>

            <!-- Section 2 Overview -->
            <section>
                <h2>Section 2: Foundations of Async Programming</h2>
                <p>The evolution of concurrency models in Python and Rust</p>
            </section>

            <!-- Python Evolution -->
            <section>
                <section>
                    <h2>Python's Async Evolution</h2>
                    <div class="mermaid">
                        flowchart LR
                            A[Twisted<br>pre-generators] --> B[Twisted<br>with generators]
                            B --> C[asyncio]
                            C --> D[Trio]
                            D --> E[anyio]
                    </div>
                    <p>A journey from explicit to implicit concurrency control</p>
                    <p><small>â†“ Press down for examples</small></p>
                </section>
                
                <!-- Twisted without generators -->
                <section>
                    <h3>Twisted (pre-generators)</h3>
                    <pre><code class="python">from twisted.internet import reactor, defer

def get_data():
    d = defer.Deferred()
    # Simulate async operation
    reactor.callLater(1, lambda: d.callback("Result"))
    return d

def handle_result(result):
    print(f"Got: {result}")
    reactor.stop()

def handle_error(failure):
    print(f"Error: {failure}")
    reactor.stop()

d = get_data()
d.addCallbacks(handle_result, handle_error)

reactor.run()</code></pre>
                    <p><small>Explicit callback-based approach using Deferreds</small></p>
                </section>
                
                <!-- Twisted with generators -->
                <section>
                    <h3>Twisted with generators (inlineCallbacks)</h3>
                    <pre><code class="python">from twisted.internet import reactor, defer
from twisted.internet.defer import inlineCallbacks

@inlineCallbacks
def async_operation():
    try:
        result1 = yield get_data()
        result2 = yield get_more_data(result1)
        print(f"Final result: {result2}")
    except Exception as e:
        print(f"Error: {e}")
    finally:
        reactor.stop()

reactor.callWhenRunning(async_operation)
reactor.run()</code></pre>
                    <p><small>Semi-implicit coroutines using yield</small></p>
                </section>
                
                <!-- asyncio -->
                <section>
                    <h3>asyncio (Python 3.5+)</h3>
                    <pre><code class="python">import asyncio

async def get_data():
    await asyncio.sleep(1)  # Simulate async operation
    return "Result"

async def main():
    try:
        result1 = await get_data()
        result2 = await process_data(result1)
        print(f"Final result: {result2}")
    except Exception as e:
        print(f"Error: {e}")

asyncio.run(main())</code></pre>
                    <p><small>Fully implicit coroutines with async/await syntax</small></p>
                </section>
                
                <!-- Trio -->
                <section>
                    <h3>Trio (Structured Concurrency)</h3>
                    <pre><code class="python">import trio

async def connect(addr):
    # ... connection attempt logic
    return socket

async def main():
    async with trio.open_nursery() as nursery:
        for addr in addresses:
            nursery.start_soon(connect, addr)
            
    # We only get here when ALL tasks are done
    print("All connections complete")</code></pre>
                    <p><small>Structured concurrency with explicit task relationships</small></p>
                </section>
                
                <!-- anyio -->
                <section>
                    <h3>anyio: Unifying asyncio and Trio</h3>
                    <pre><code class="python">import anyio

async def main():
    async with anyio.create_task_group() as tg:
        for addr in addresses:
            tg.start_soon(connect_to, addr)
    
    # Works on both asyncio and trio backends
    
# Run with asyncio backend
anyio.run(main, backend="asyncio")

# Or with trio backend
anyio.run(main, backend="trio")</code></pre>
                    <p><small>2.4M monthly downloads, powers httpx, FastAPI, Pydantic AI</small></p>
                </section>
            </section>

            <!-- Memory Safety vs Liveness -->
            <section>
                <h2>Memory Safety vs Liveness</h2>
                <div class="two-columns">
                    <div class="column">
                        <h3>Memory Safety</h3>
                        <ul>
                            <li>Prevents undefined behavior from invalid memory access</li>
                            <li>Avoids segmentation faults, buffer overflows, use-after-free</li>
                            <li>Focus of Rust's borrow checker</li>
                        </ul>
                    </div>
                    <div class="column">
                        <h3>Liveness</h3>
                        <ul>
                            <li>Ensures program makes progress</li>
                            <li>Prevents deadlocks, livelocks, infinite loops</li>
                            <li>Focus of structured concurrency</li>
                        </ul>
                    </div>
                </div>
                <p>Languages typically prioritize one over the other</p>
            </section>

            <!-- Memory Safety in Python and Rust -->
            <section>
                <h2>How Memory Safety is Achieved</h2>
                <table>
                    <tr>
                        <th>Python</th>
                        <th>Rust</th>
                    </tr>
                    <tr>
                        <td>Automatic memory management via garbage collection</td>
                        <td>Compile-time ownership and lifetime checking</td>
                    </tr>
                    <tr>
                        <td>Object reference counting + cycle detection</td>
                        <td>No garbage collector for most code</td>
                    </tr>
                    <tr>
                        <td>Runtime checks (IndexError, KeyError, etc.)</td>
                        <td>Static analysis prevents most errors at compile time</td>
                    </tr>
                    <tr>
                        <td>No direct pointer manipulation in pure Python</td>
                        <td>Safe abstractions around pointers (references, Box, etc.)</td>
                    </tr>
                    <tr>
                        <td>C extensions can introduce memory safety issues</td>
                        <td>Unsafe blocks explicitly mark potential issues</td>
                    </tr>
                </table>
            </section>

            <!-- Structured Concurrency: Liveness Solutions -->
            <section>
                <section>
                    <h2>Structured Concurrency: Solving Liveness</h2>
                    <div class="two-columns">
                        <div class="column">
                            <h3>Trio's Model</h3>
                            <img src="images/trio-scope-exit.png" alt="Trio scope exiting diagram" style="height: 40vh;">
                            <p><small>"All tasks are equal"</small></p>
                        </div>
                        <div class="column">
                            <h3>Key Principles</h3>
                            <ul>
                                <li>Tasks tied to lexical scopes</li>
                                <li>Automatic cancellation on scope exit</li>
                                <li>Error propagation to parent</li>
                                <li>Child tasks cannot outlive parent scope</li>
                            </ul>
                        </div>
                    </div>
                    <p><small>â†“ Press down for liveness guarantee examples</small></p>
                </section>

                <section>
                    <h2>The Infinite Loop Problem</h2>
                    <div class="two-columns">
                        <div class="column">
                            <h3>Problem in Tokio</h3>
                            <pre><code class="rust">// This will never complete if the socket
// continues to receive data
let results = tokio::join!(
    infinite_socket_reader(&mut socket),
    some_other_task()
);

// This line is never reached</code></pre>
                        </div>
                        <div class="column">
                            <h3>Solution in Trio</h3>
                            <pre><code class="python">async with trio.move_on_after(30):
    async with trio.open_nursery() as nursery:
        nursery.start_soon(infinite_socket_reader)
        nursery.start_soon(some_other_task)
        
# Control reaches here after at most 30 seconds</code></pre>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>Cancellation Systems</h2>
                    <div class="two-columns">
                        <div class="column">
                            <h3>Trio's Approach</h3>
                            <ul>
                                <li>First-class cancellation scopes</li>
                                <li>Automatic propagation</li>
                                <li>Built-in timeout mechanisms</li>
                                <li>Checkpoint system for cancellation detection</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h3>Tokio's Approach</h3>
                            <ul>
                                <li>Manual cancellation via channels or drop</li>
                                <li>Explicit timeout wrappers</li>
                                <li>select! for racing between operations</li>
                                <li>No automatic propagation</li>
                            </ul>
                        </div>
                    </div>
                </section>
            </section>

            <!-- Rust's Thread Safety -->
            <section>
                <section>
                    <h2>Rust's Thread Safety Guarantees</h2>
                    <div class="two-columns">
                        <div class="column">
                            <h3>Send Trait</h3>
                            <ul>
                                <li>Types that can be transferred between threads</li>
                                <li>Example: Vec&lt;i32&gt; is Send</li>
                                <li>Counter-example: Rc&lt;T&gt; is not Send</li>
                            </ul>
                        </div>
                        <div class="column">
                            <h3>Sync Trait</h3>
                            <ul>
                                <li>Types that can be shared between threads</li>
                                <li>Example: Mutex&lt;T&gt; is Sync</li>
                                <li>Counter-example: RefCell&lt;T&gt; is not Sync</li>
                            </ul>
                        </div>
                    </div>
                    <p><small>â†“ Press down for examples</small></p>
                </section>

                <section>
                    <h2>Static Lifetime Requirements</h2>
                    <div class="two-columns">
                        <div class="column">
                            <h3>Unstructured Tasks</h3>
                            <pre><code class="rust">// Requires 'static lifetime for data
tokio::spawn(async move {
    println!("Data: {:?}", data);
});

// Common workarounds:
// 1. Arc::clone()
// 2. String::to_owned()
// 3. Sometimes Box::leak() (memory leak)</code></pre>
                        </div>
                        <div class="column">
                            <h3>Structured Alternatives</h3>
                            <pre><code class="rust">// No 'static requirement
tokio::task::scope(|s| {
    let data = vec![1, 2, 3]; // Non-'static!
    
    s.spawn(async move {
        println!("Data: {:?}", data);
    });
});

// Or with join!
let results = tokio::join!(
    process_data(&data),
    other_task()
);</code></pre>
                        </div>
                    </div>
                </section>
            </section>

            <!-- Comparison of Parallelism Models -->
            <section>
                <h2>Parallelism Approaches</h2>
                <div class="two-columns">
                    <div class="column">
                        <h3>Python</h3>
                        <ul>
                            <li>concurrent.futures.ProcessPoolExecutor</li>
                            <li>Multiprocessing module</li>
                            <li>Future: InterpreterPoolExecutor with free-threaded Python</li>
                            <li>Limited by GIL in threaded code</li>
                        </ul>
                    </div>
                    <div class="column">
                        <h3>Rust</h3>
                        <ul>
                            <li>std::thread for raw threads</li>
                            <li>Rayon for data parallelism</li>
                            <li>Tokio for task concurrency + thread pools</li>
                            <li>No GIL, true parallelism</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Rayon Example -->
            <section>
                <h2>Rayon: Data Parallelism in Rust</h2>
                <pre><code class="rust">use rayon::prelude::*; 

fn main() { 
    let data: Vec<u32> = (0..1000).collect();
    
    // Sequential
    let sum1: u32 = data.iter().map(|&x| expensive_calculation(x)).sum();
    
    // Parallel - just change iter() to par_iter()
    let sum2: u32 = data.par_iter().map(|&x| expensive_calculation(x)).sum();
    
    assert_eq!(sum1, sum2);
}</code></pre>
                <p>Drop-in parallelism with minimal code changes</p>
            </section>

            <!-- Developer Experience -->
            <section>
                <h2>Developer Experience Tradeoffs</h2>
                <table>
                    <tr>
                        <th>Aspect</th>
                        <th>Python Approach</th>
                        <th>Rust Approach</th>
                    </tr>
                    <tr>
                        <td>Cognitive Load</td>
                        <td>Lower - hides many concurrency details</td>
                        <td>Higher - explicit about lifetimes, ownership</td>
                    </tr>
                    <tr>
                        <td>Error Detection</td>
                        <td>Runtime - errors happen during execution</td>
                        <td>Compile time - most errors caught before running</td>
                    </tr>
                    <tr>
                        <td>Mental Model</td>
                        <td>Simple but sometimes hides important details</td>
                        <td>Complex but precise about behavior</td>
                    </tr>
                    <tr>
                        <td>Composability</td>
                        <td>High with structured concurrency</td>
                        <td>Requires more explicit composition</td>
                    </tr>
                    <tr>
                        <td>Performance Control</td>
                        <td>Limited fine-grained control</td>
                        <td>Extensive control over execution details</td>
                    </tr>
                </table>
            </section>

            <!-- Key Insights -->
            <section>
                <h2>Key Insights</h2>
                <ol>
                    <li><strong>Memory Safety vs. Liveness:</strong> Languages typically excel at one</li>
                    <li><strong>Structured Concurrency:</strong> Provides liveness guarantees Python developers expect</li>
                    <li><strong>Thread Safety:</strong> Rust's type system makes concurrency bugs compile errors</li>
                    <li><strong>Developer Experience:</strong> Tradeoffs between simplicity and control</li>
                    <li><strong>Free-threaded Python on Tokio:</strong> Would need to bridge these different models</li>
                </ol>
            </section>

            <!-- Bridging the Gap -->
            <section>
                <h2>Challenges for Free-threaded Python atop Tokio</h2>
                <ul>
                    <li>Preserving Trio-style structured concurrency guarantees</li>
                    <li>Adding cancellation propagation to Tokio's model</li>
                    <li>Maintaining Python's ergonomics with Rust's performance</li>
                    <li>Ensuring thread-safety without complex annotations</li>
                    <li>Addressing the infinite loop/liveness problem</li>
                </ul>
                <p>The ideal system would combine Python's structured concurrency model with Rust's efficient threading</p>
            </section>

            <!-- SECTION 3: CASE STUDY - GENETIC ALGORITHM -->
            <section>
                <h2>Case Study: Genetic Algorithm</h2>
                <h3>"A Million Monkeys at a Million Typewriters"</h3>
                <p>Evolving random text into Hamlet quotes</p>
                <blockquote>"Brevity is the soul of wit"</blockquote>
                <p><em>(Also the soul of optimization)</em></p>
            </section>

            <section>
                <h2>Key Parameters in Genetic Algorithms</h2>
                <ul>
                    <li><strong>Population size:</strong> Controls exploration breadth</li>
                    <li><strong>Mutation rate:</strong> Enables discovering new traits</li>
                    <li><strong>Selection pressure:</strong> Balances exploitation vs exploration</li>
                    <li><strong>Crossover rate:</strong> Combines successful traits</li>
                </ul>
            </section>

            <section>
                <h2>Visualizing Crossover in High-Dimensional Space</h2>
                <ul>
                    <li>40-character string = 40D vector</li>
                    <li>Single-point crossover at position 15</li>
                    <li>Combining orthogonal projections</li>
                </ul>
                <div class="chart-placeholder">
                    [Visualization of string crossover in vector space]
                </div>
            </section>

            <!-- Python Implementation section - vertical slide group -->
            <section>
                <!-- Main/top slide with concepts -->
                <section>
                    <h2>Sequential Implementation</h2>
                    <h3>Core Components</h3>
                    <ul>
                        <li><strong>Population Management:</strong> Random initialization and generational tracking</li>
                        <li><strong>Fitness Calculation:</strong> Character-by-character matching</li>
                        <li><strong>Selection:</strong> Fitness-proportional sampling</li>
                        <li><strong>Reproduction:</strong> Crossover and mutation operations</li>
                    </ul>
                    <p><small>â†“ Press down for code implementation</small></p>
                </section>

                <!-- Code slide below -->
                <section>
                    <h2>Basic Genetic Algorithm</h2>
                    <pre><code class="python">def evolve_text(target, pop_size=1000, mutation_rate=0.01):
    # Create initial population
    population = [''.join(random.choice(CHARS) 
                for _ in range(len(target))) 
                for _ in range(pop_size)]
    
    generation = 0
    best_fit = 0
    
    while best_fit < len(target):
        # Calculate fitness for each member
        fitness_scores = [calculate_fitness(p, target) 
                         for p in population]
        
        # Selection and reproduction
        new_population = []
        for _ in range(pop_size):
            parent1 = selection(population, fitness_scores)
            parent2 = selection(population, fitness_scores)
            child = crossover(parent1, parent2)
            child = mutate(child, mutation_rate)
            new_population.append(child)
            
        population = new_population
        # ... rest of implementation</code></pre>
                    <p><small>Sequential implementation - basis for our parallel optimizations</small></p>
                </section>
            </section>

            <section>
                <h2>Initial Parallelization Strategy</h2>
                <div class="chart-placeholder">
                    [Diagram: Parallel fitness calculation pipeline]
                </div>
                <p>Pipeline model: fitness â†’ selection â†’ crossover/mutation</p>
            </section>

            <section>
                <h2>Identifying Parallelism Opportunities</h2>
                <ul>
                    <li><strong>Fitness calculation:</strong> Embarrassingly parallel</li>
                    <li><strong>Selection:</strong> Limited parallelism (requires sorting)</li>
                    <li><strong>Reproduction:</strong> Moderately parallel</li>
                </ul>
            </section>

            <!-- Tournament Selection - vertical slide group -->
            <section>
                <!-- Main/top slide -->
                <section>
                    <h2>Tournament Selection</h2>
                    <ul>
                        <li>Alternative to global sorting</li>
                        <li>Independent tournaments = better parallelism</li>
                        <li>Tunable selection pressure</li>
                        <li>Natural fit for async/await pattern</li>
                    </ul>
                    <p><small>â†“ Press down for implementation</small></p>
                </section>

                <!-- Code slide below -->
                <section>
                    <h2>Async Tournament Selection</h2>
                    <pre><code class="python">async def tournament_selection(population, fitness_func, tournament_size=5):
    # Select random individuals for tournament
    tournament = random.sample(population, tournament_size)
    
    # Calculate fitness in parallel
    tasks = [asyncio.create_task(fitness_func(ind)) 
             for ind in tournament]
    fitness_values = await asyncio.gather(*tasks)
    
    # Return the winner
    return tournament[fitness_values.index(max(fitness_values))]</code></pre>
                    <p><small>Concurrent fitness evaluation with minimal synchronization</small></p>
                </section>
            </section>

            <section>
                <h2>Asynchronous Genetic Algorithm Design</h2>
                <div class="mermaid">
                    flowchart LR
                        A[Population] --> B{Tournament<br/>Selection}
                        B -->|Winners| C[Crossover]
                        C --> D[Mutation]
                        D --> E[Fitness<br/>Evaluation]
                        E -->|Next Gen| A
                        E -->|Best| F[Output]
                </div>
                <p>Reduced synchronization points</p>
            </section>

            <!-- Optimized Population Data Structure - vertical slide group -->
            <section>
                <!-- Main/top slide -->
                <section>
                    <h2>Optimized Population Data Structure</h2>
                    <ul>
                        <li>Single central dictionary with generation index</li>
                        <li>Memory efficiency considerations</li>
                        <li>Automatic cleanup of older generations</li>
                    </ul>
                    <p><small>â†“ Press down for implementation details</small></p>
                </section>

                <!-- Code slide below -->
                <section>
                    <h2>Implementation Details</h2>
                    <pre><code class="python">class Population:
    def __init__(self, max_generations_to_keep=3):
        self.members = {}  # {generation: [members]}
        self.max_generations = max_generations_to_keep
        self.current_generation = 0
        self.lock = asyncio.Lock()
        
    async def add_member(self, member):
        async with self.lock:
            if member.generation not in self.members:
                self.members[member.generation] = []
            self.members[member.generation].append(member)
            
    async def cleanup_old_generations(self):
        async with self.lock:
            generations = sorted(self.members.keys())
            to_remove = generations[:-self.max_generations] \
                if len(generations) > self.max_generations else []
            for gen in to_remove:
                del self.members[gen]</code></pre>
                    <p><small>Thread-safe population management with generational garbage collection</small></p>
                </section>
            </section>

            <section>
                <h2>Performance Comparison</h2>
                <div class="chart-placeholder">
                    [Flame graph comparing Python vs Rust implementation]
                </div>
                <p>Key bottlenecks identified:</p>
                <ul>
                    <li>Fitness calculation in Python</li>
                    <li>GIL contention</li>
                    <li>Memory allocation patterns</li>
                </ul>
            </section>

            <!-- SECTION 4: CONCLUSION -->
            <section>
                <h2>Evolution of Our Implementation</h2>
                <ol>
                    <li>Initial implementation</li>
                    <li>Understanding execution model</li>
                    <li>Tournament selection for parallelism</li>
                    <li>Generation-aware asynchronous approach</li>
                </ol>
            </section>

            <section>
                <h2>Returning to the Initial Question</h2>
                <blockquote>
                    "Would Pythonistas benefit from free-threaded Python atop the Tokio runtime?"
                </blockquote>
                <p>My thoughts:</p>
                <ul>
                    <li>Potential for substantial performance gains</li>
                    <li>Challenges in API design and mental model</li>
                    <li>Integration complexity considerations</li>
                </ul>
            </section>

            <section>
                <h2>Next Steps and Resources</h2>
                <div class="two-columns">
                    <div class="column">
                        <h3>Communities</h3>
                        <ul>
                            <li>Discord: #python-rust-interop</li>
                            <li>Reddit: r/rustpython</li>
                            <li>GitHub: PyO3 organization</li>
                        </ul>
                    </div>
                    <div class="column">
                        <h3>Learning Resources</h3>
                        <ul>
                            <li>Rust Book</li>
                            <li>Tokio Documentation</li>
                            <li>asyncio Documentation</li>
                        </ul>
                    </div>
                </div>
                <p>Your Contact Information:</p>
                <p>email@example.com | @twitter_handle | github.com/username</p>
            </section>

            <section>
                <h2>Sources and Credits</h2>
                <ul>
                    <li>Rust programming language book, rustlings, rust by example</li>
                    <li>Tokio glossary</li>
                    <li>Arden labs: Fearless Concurrency in Rust series</li>
                    <li>Piotr's performance benchmarking</li>
                    <li>David Hewitt Rust Nation UK talk</li>
                    <li>Evgenii Seliversov: Parallel Programming in Rust techniques</li>
                    <li>Personal correspondence</li>
                    <li>Claude assistance</li>
                </ul>
            </section>

            <section>
                <h1>Thank You!</h1>
                <h3>Questions?</h3>
            </section>
        </div>
    </div>

    <!-- Ferris Runner -->
    <div class="ferris-runner">
        <img src="https://rustacean.net/assets/rustacean-flat-happy.svg" alt="Ferris">
    </div>

    <!-- Load all scripts at the end of body -->
    <!-- Core libraries -->
    <script src="https://cdn.jsdelivr.net/npm/highlight.js@11.8.0/highlight.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/highlight.js@11.8.0/languages/python.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/highlight.js@11.8.0/languages/rust.min.js"></script>
    
    <!-- Reveal.js and its plugins -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/plugin/markdown/markdown.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/plugin/math/math.js"></script>
    
    <!-- Mermaid plugin -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js-mermaid-plugin@11.4.1/plugin/mermaid/mermaid.min.js"></script>
    
    <!-- Custom script - load last -->
    <script src="script.js"></script>
</body>
</html> 
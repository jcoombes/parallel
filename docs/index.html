<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Asynchronous Programming in Python and Rust - PyCon DE & PyData 2025</title>
    
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@400;500;600&display=swap" rel="stylesheet">
    
    <!-- Reveal.js CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/dist/reset.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/dist/theme/black.css" id="theme">
    
    <!-- Highlight.js and theme -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.8.0/styles/vs2015.css">
    
    <!-- Custom styles -->
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- SECTION 1: INTRODUCTION -->
            <section>
                <h1>ðŸ¦€ RÃ¼stzeit</h1>
                <h3>Asynchronous and Concurrent Optimisation in Python and Rust</h3>
                <p>Jamie Coombes</p>
                <p class="conference-info">PyCon DE & PyData 2025</p>
                <p class="conference-track">PyCon: Rust</p>
                <p class="conference-date">23. - 25. April 2025</p>
                <p class="location">Darmstadtium, Darmstadt, Deutschland</p>
            </section>

            <!-- Talk Overview -->
            <section>
                <ol>
                    <li><strong>Main Talk Question</strong>
                        <p>Python/Rust Interop and David Hewitt's free-threaded python atop Tokio question.</p>
                    </li>
                    <li><strong>Asynchronous Concurrency</strong>
                        <p>Exploring async foundations through Rayon and Tokio in Rust, compared with Python's approaches.</p>
                    </li>
                    <li><strong>Case Study: A million monkeys at a million typewriters</strong>
                        <p>Parallelising and benchmarking a genetic algorithm for evolving text into Shakespeare quotes.</p>
                    </li>
                    <li><strong>Conclusion</strong>
                        <p>Exploring the future of Python/Rust Interop and the potential for free-threaded Python atop Tokio.</p>
                    </li>
                </ol>
            </section>

            <!-- Rust/Python Interop Growth -->
            <section>
                <section>
                    <h2>Python/Rust Interop Growth in 2025</h2>
                    <img src="images/pypi-rust-growth.png" alt="Growth of PyPI packages with Rust components.">
                </section>
                <section>
                    <ul>
                        <h2>Rust/Python Interop Growth in 2025</h2>
                        <li>Rust language as preferred developer-experience for writing native python extensions.</li>
                        <li>RIIR - Rewrite It In Rust - for 10x-80x* performance improvements</li>
                        <li>PyO3 is the most popular Rust library for Python interop.</li>
                        <li>Maturin is the most popular tool for building and distributing Rust Python packages.</li>
                        <small>*performance benchmarking is hard, see flame graphs later</small>
                    </ul>
                </section>
            </section>
    

            <!-- Inciting Question -->
            <section>
                <section>
                    <h2>The Main Talk Questions</h2>
                    <blockquote>
                        "Do we want to expose the tokio event loop as the basis for a Python async runtime?"
                    </blockquote>
                    <p>â€” David Hewitt <a href="https://x.com/davidhewittdev">@davidhewittdev</a> - Rust Nation UK 2025</p>
                    <blockquote>
                        "How will free-threaded Python lead to better parallel code?"
                    </blockquote>
                    <p>- me, today, right now</p>
                </section>

                <!-- Async Runtime Diagram -->
                <section>
                    <h2>Free-threaded Python + Tokio = ??</h2>
                    <img src="assets/tokio-based-python-runtime-dark.svg" alt="Comparison of single-threaded asyncio vs multi-threaded tokio-based Python runtime">
                </section>
            </section>

            <section>
                <h2>Free Threading in Python</h2>
                <p>Python 3.13 shipped an <italic>experimental</italic> variant without the "Global Interpreter Lock"
                <ul>
                    <li>Expected to become the default in a few years</li>
                    <li>PyO3 supports 3.13t fully</li>
                    <li>Rust's Send + Sync traits make it straightforward to reason about threading issues</li>
                </ul>
            </section>

            <!-- Tokio-based Python Runtime benefits-->
            <section>
                <section>
                    <h2>A multi-threaded tokio-based Python runtime could be...</h2>
                </section>
                <section>
                    <h2>Memory Efficient (and hence scalable)</h2>
                        <img src="images/tokio-memory-usage-10k.png" alt="Comparison of memory usage between tokio and other async runtimes">
                        <p>Image source: <a href="https://pkolaczk.github.io/memory-consumption-of-async/">Pkolaczk's memory-consumption-of-async</a></p>
                </section>
                <section>
                    <h2>...well past os.cpu_count() threads</h2>
                    <div class="chart-placeholder">
                        <img src="images/tokio-memory-usage-1M.png" alt="Comparison of memory usage between tokio and other async runtimes">
                        <p>Image source: <a href="https://pkolaczk.github.io/memory-consumption-of-async/">Pkolaczk's memory-consumption-of-async</a></p>
                    </div>
                </section>
                <section>
                    <h2>Typed to prevent data races</h2>
                    <ul>
                        <li>Rust's ownership model guarantees thread safety at compile time</li>
                        <li>Rich ecosystem of threading primitives (std::sync, parking_lot, crossbeam)</li>
                        <li>Threading can complement or replace async patterns</li>
                    </ul>
                    <p><small>Note: While Rust prevents data races, it can't prevent all concurrency bugs (deadlocks, algorithmic issues)</small></p>
                </section>
                <section>
                    <h2>Faster</h2>
                    <p>so you add :rocket: to your project README</p>
                    <img src="/images/rust-python-perf.png" alt="Rocket emoji">
                </section>
            </section>

            <section>
                <section>
                    <h2>okay, but I really like python</h2>
                    <p>what are the multi-threaded async alternatives?</p>
                    <p>well there is threado...</p>
                </section>
                <section>
                    <h2>but threado is deado</h2>
                    <div class="chart-placeholder">
                        <img src="images/threado-dead.png" alt="threado is super dead">
                    </div>
                    <p>maybe ask <a href="@dabeaz@mastodon.social">@dabeaz</a> to bring it back for 3.14t or 3.15t?</p>
                    <p>or just do dabeaz's <a href="https://www.dabeaz.com/summer.html">summer of rust</a> and create a tokio-based python runtime?</p>
                </section>
            </section>

            <section>
                <h2>How do rust and python think about concurrency and parallelism?</h2>
            </section>

            <!-- Section 2 Overview -->
            <section>
                <h2>Section 2: Foundations of Async Programming</h2>
                <p>The evolution of concurrency models in Python and Rust</p>
            </section>

            <!-- Python Evolution -->
            <section>
                <section>
                    <h2>Python's Async Evolution</h2>
                    <div class="mermaid">
                        flowchart LR
                            A[Twisted<br>pre-generators] --> B[Twisted<br>with generators]
                            B --> C[asyncio]
                            C --> D[Trio]
                            D --> E[anyio]
                    </div>
                    <p>A journey from explicit to implicit concurrency control</p>
                    <p><small>â†“ Press down for examples</small></p>
                </section>
                
                <!-- asyncio -->
                <section>
                    <h3>asyncio (Python 3.5+)</h3>
                    <pre><code class="python">import asyncio

async def get_data():
    await asyncio.sleep(1)  # Simulate async operation
    return "Result"

async def main():
    try:
        result1 = await get_data()
        result2 = await process_data(result1)
        print(f"Final result: {result2}")
    except Exception as e:
        print(f"Error: {e}")

asyncio.run(main())</code></pre>
                    <p><small>Fully implicit coroutines with async/await syntax</small></p>
                </section>
                
                <!-- Trio -->
                <section>
                    <h3>Trio (Structured Concurrency)</h3>
                    <pre><code class="python">import trio

async def connect(addr):
    # ... connection attempt logic
    return socket

async def main():
    async with trio.open_nursery() as nursery:
        for addr in addresses:
            nursery.start_soon(connect, addr)
            
    # We only get here when ALL tasks are done
    print("All connections complete")</code></pre>
                    <p><small>Structured concurrency with explicit task relationships</small></p>
                </section>
                
                <!-- anyio -->
                <section>
                    <h3>anyio: Unifying asyncio and Trio</h3>
                    <pre><code class="python">import anyio

async def main():
    async with anyio.create_task_group() as tg:
        for addr in addresses:
            tg.start_soon(connect_to, addr)
    
    # Works on both asyncio and trio backends
    
# Run with asyncio backend
anyio.run(main, backend="asyncio")

# Or with trio backend
anyio.run(main, backend="trio")</code></pre>
                    <p><small>2.4M monthly downloads, powers httpx, FastAPI, Pydantic AI</small></p>
                </section>
            </section>

            <!-- Memory Safety vs Liveness -->
            <section>
                <h2>Memory Safety vs Liveness</h2>
                <div class="two-columns">
                    <div class="column">
                        <h3>Memory Safety</h3>
                        <ul>
                            <li>Prevents undefined behavior from invalid memory access</li>
                            <li>Avoids segmentation faults, buffer overflows, use-after-free</li>
                            <li>Focus of Rust's borrow checker</li>
                        </ul>
                    </div>
                    <div class="column">
                        <h3>Liveness</h3>
                        <ul>
                            <li>Ensures program makes progress</li>
                            <li>Prevents deadlocks, livelocks, infinite loops</li>
                            <li>Focus of structured concurrency</li>
                        </ul>
                    </div>
                </div>
                <p>Languages typically prioritize one over the other</p>
            </section>

            <!-- Memory Safety in Python and Rust -->
            <section>
                <h2>How Memory Safety is Achieved</h2>
                <table>
                    <tr>
                        <th>Python</th>
                        <th>Rust</th>
                    </tr>
                    <tr>
                        <td>Automatic memory management via garbage collection</td>
                        <td>Compile-time ownership and lifetime checking</td>
                    </tr>
                    <tr>
                        <td>Object reference counting + cycle detection</td>
                        <td>No garbage collector for most code</td>
                    </tr>
                    <tr>
                        <td>Runtime checks (IndexError, KeyError, etc.)</td>
                        <td>Static analysis prevents most errors at compile time</td>
                    </tr>
                    <tr>
                        <td>No direct pointer manipulation in pure Python</td>
                        <td>Safe abstractions around pointers (references, Box, etc.)</td>
                    </tr>
                    <tr>
                        <td>C extensions can introduce memory safety issues</td>
                        <td>Unsafe blocks explicitly mark potential issues</td>
                    </tr>
                </table>
            </section>

            <!-- Structured Concurrency: Liveness Solutions -->
            <section>
                <section>
                    <h2>Structured Concurrency: Solving Liveness</h2>
                    <div class="two-columns">
                        <div class="column">
                            <h3>Trio's Model</h3>
                            <img src="images/trio-scope-exit.png" alt="Trio scope exiting diagram" style="height: 40vh;">
                            <p><small>"All tasks are equal"</small></p>
                        </div>
                        <div class="column">
                            <h3>Key Principles</h3>
                            <ul>
                                <li>Tasks tied to lexical scopes</li>
                                <li>Automatic cancellation on scope exit</li>
                                <li>Error propagation to parent</li>
                                <li>Child tasks cannot outlive parent scope</li>
                            </ul>
                        </div>
                    </div>
                    <p><small>â†“ Press down for liveness guarantee examples</small></p>
                </section>

                <section>
                    <h2>The Infinite Loop Problem</h2>
                    <div class="two-columns">
                        <div class="column">
                            <h3>Problem in Tokio</h3>
                            <pre><code class="rust">// This will never complete if the socket
// continues to receive data
let results = tokio::join!(
    infinite_socket_reader(&mut socket),
    some_other_task()
);

// This line is never reached</code></pre>
                        </div>
                        <div class="column">
                            <h3>Solution in Trio</h3>
                            <pre><code class="python">async with trio.move_on_after(30):
    async with trio.open_nursery() as nursery:
        nursery.start_soon(infinite_socket_reader)
        nursery.start_soon(some_other_task)
        
# Control reaches here after at most 30 seconds</code></pre>
                        </div>
                    </div>
                </section>
            </section>

            <!-- Rust's Thread Safety -->
            <section>
                <h2>Rust's Thread Safety Guarantees</h2>
                <div class="two-columns">
                    <div class="column">
                        <h3>Send Trait</h3>
                        <ul>
                            <li>Types that can be transferred between threads</li>
                            <li>Example: Vec&lt;i32&gt; is Send</li>
                            <li>Counter-example: Rc&lt;T&gt; is not Send</li>
                        </ul>
                    </div>
                    <div class="column">
                        <h3>Sync Trait</h3>
                        <ul>
                            <li>Types that can be shared between threads</li>
                            <li>Example: Mutex&lt;T&gt; is Sync</li>
                            <li>Counter-example: RefCell&lt;T&gt; is not Sync</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Comparison of Parallelism Models -->
            <section>
                <h2>Parallelism Approaches</h2>
                <div class="two-columns">
                    <div class="column">
                        <h3>Python</h3>
                        <ul>
                            <li>concurrent.futures.ProcessPoolExecutor</li>
                            <li>Multiprocessing module</li>
                            <li>Future: InterpreterPoolExecutor with free-threaded Python</li>
                            <li>Limited by GIL in threaded code</li>
                        </ul>
                    </div>
                    <div class="column">
                        <h3>Rust</h3>
                        <ul>
                            <li>std::thread for raw threads</li>
                            <li>Rayon for data parallelism</li>
                            <li>Tokio for task concurrency + thread pools</li>
                            <li>No GIL, true parallelism</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Rayon Example -->
            <section>
                <h2>Rayon: Data Parallelism in Rust</h2>
                <pre><code class="rust">use rayon::prelude::*; 

fn main() { 
    let data: Vec<u32> = (0..1000).collect();
    
    // Sequential
    let sum1: u32 = data.iter().map(|&x| expensive_calculation(x)).sum();
    
    // Parallel - just change iter() to par_iter()
    let sum2: u32 = data.par_iter().map(|&x| expensive_calculation(x)).sum();
    
    assert_eq!(sum1, sum2);
}</code></pre>
                <p>Drop-in parallelism with minimal code changes</p>
            </section>

            <!-- Key Insights -->
            <section>
                <h2>Key Insights</h2>
                <ol>
                    <li><strong>Memory Safety vs. Liveness:</strong> Languages typically excel at one</li>
                    <li><strong>Structured Concurrency:</strong> Provides liveness guarantees Python developers expect</li>
                    <li><strong>Thread Safety:</strong> Rust's type system makes concurrency bugs compile errors</li>
                    <li><strong>Developer Experience:</strong> Tradeoffs between simplicity and control</li>
                    <li><strong>Free-threaded Python on Tokio:</strong> Would need to bridge these different models</li>
                </ol>
            </section>

            <!-- Bridging the Gap -->
            <section>
                <h2>Challenges for Free-threaded Python atop Tokio</h2>
                <ul>
                    <li>Preserving Trio-style structured concurrency guarantees</li>
                    <li>Adding cancellation propagation to Tokio's model</li>
                    <li>Maintaining Python's ergonomics with Rust's performance</li>
                    <li>Ensuring thread-safety without complex annotations</li>
                    <li>Addressing the infinite loop/liveness problem</li>
                </ul>
                <p>The ideal system would combine Python's structured concurrency model with Rust's efficient threading</p>
            </section>

            <!-- SECTION 3: CASE STUDY - GENETIC ALGORITHM -->
            <section>
                <h2>Case Study: Genetic Algorithm</h2>
                <h3>"A Million Monkeys at a Million Typewriters"</h3>
                <p>Evolving random text into Shakespeare quotes</p>
                <blockquote>"Brevity is the soul of wit"</blockquote>
                <p><em>(Also the soul of optimization)</em></p>
            </section>

            <section>
                <h2>Key Components of Our Genetic Algorithm</h2>
                <ul>
                    <li><strong>Population:</strong> Collection of candidate solutions (text strings)</li>
                    <li><strong>Fitness Function:</strong> Character-by-character matching with target</li>
                    <li><strong>Selection:</strong> Tournament selection for choosing parents</li>
                    <li><strong>Crossover:</strong> Combining parent strings at random split points</li>
                    <li><strong>Mutation:</strong> Random character changes to maintain diversity</li>
                </ul>
                <p>Our goal: Evolve gibberish text into Shakespeare quotes efficiently</p>
            </section>

            <section>
                <h2>Tokio Architecture for the GA</h2>
                <div class="mermaid">
                    flowchart TD
                        A[Main Evolution Loop] --> B[Population]
                        B --> C[Parallel Fitness Calculation]
                        C --> D[JoinSet for Tasks]
                        D --> E[Worker Thread Pool]
                        B --> F[Tournament Selection]
                        F --> G[Crossover/Mutation]
                        G --> B
                </div>
                <p>Multi-threaded parallel processing with structured task management</p>
            </section>

            <!-- JoinSet for Parallel Fitness -->
            <section>
                <h2>JoinSet: Parallel Fitness Calculation</h2>
                <pre><code class="rust">pub async fn calculate_fitness_parallel(&mut self) -> Vec<Individual> {
    let target = Arc::new(self.target.clone());
    let mut join_set = JoinSet::new();

    // Split population into chunks for each worker
    let chunks: Vec<Vec<Individual>> = self
        .population
        .chunks(self.population.len() / num_cpus::get().max(1))
        .map(|chunk| chunk.to_vec())
        .collect();

    // Process each chunk in parallel
    for chunk in chunks {
        let target_clone = Arc::clone(&target);
        join_set.spawn(async move {
            let mut results = Vec::with_capacity(chunk.len());
            for mut individual in chunk {
                individual.calculate_fitness(&target_clone);
                results.push(individual);
            }
            results
        });
    }

    // Collect results
    let mut all_results = Vec::with_capacity(self.population.len());
    while let Some(result) = join_set.join_next().await {
        if let Ok(chunk_results) = result {
            all_results.extend(chunk_results);
        }
    }

    all_results
}</code></pre>
                <p>Adaptive chunking ensures efficient distribution across CPU cores</p>
            </section>

            <!-- Message Passing -->
            <section>
                <h2>Message Passing for Progress Reporting</h2>
                <pre><code class="rust">// Channel for reporting updates
let (tx, mut rx) = mpsc::channel(100);

// Spawn a separate task for handling I/O
if !benchmark_mode {
    let progress_clone = Arc::clone(&progress);
    tokio::spawn(async move {
        while let Some((generation, fitness)) = rx.recv().await {
            let mut progress = progress_clone.lock().await;
            progress.push((generation, fitness));

            // Print current status
            println!("Generation {}: Best fitness {:.4}", 
                    generation, fitness);
        }
    });
}</code></pre>
                <p>Decouples computation from I/O to avoid blocking the worker threads</p>
            </section>

            <section>
                <h2>Performance Analysis</h2>
                <img src="docs/images/genetic-algorithm-benchmark.png" alt="Criterion benchmark results">
                <ul>
                    <li><strong>Text Size Impact:</strong> Linear scaling with input length</li>
                    <li><strong>Thread Count:</strong> Near-linear speedup up to CPU core count</li>
                    <li><strong>Chunk Size:</strong> Optimal at population_size / (2 Ã— num_cpus)</li>
                    <li><strong>Memory Usage:</strong> ~500 bytes per individual + task overhead</li>
                </ul>
            </section>

            <section>
                <h2>Benchmark Results</h2>
                <ul>
                    <li><strong>Rust: ~7ms per generation</strong></li>
                    <li><strong>Python: ~70ms per generation</strong></li>
                    <li>10x speedup from:
                        <ul>
                            <li>Zero-cost abstractions</li>
                            <li>Efficient parallel execution</li>
                            <li>No GIL contention</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <section>
                <h2>Key Design Decisions</h2>
                <div class="two-columns">
                    <div class="column">
                        <h3>Taskification Strategy</h3>
                        <ul>
                            <li>Chunk-based vs individual-based tasks</li>
                            <li>One task per CPU core minimizes overhead</li>
                            <li>Work-stealing naturally balances uneven chunks</li>
                            <li>Fixed thread pool matches system CPUs</li>
                        </ul>
                    </div>
                    <div class="column">
                        <h3>Data Sharing Approach</h3>
                        <ul>
                            <li>Arc for shared immutable data</li>
                            <li>Clone for task-local mutable data</li>
                            <li>Channels for progress communication</li>
                            <li>Mutex only for visualization data</li>
                        </ul>
                    </div>
                </div>
            </section>

            <section>
                <h2>Case Study Conclusions</h2>
                <ul>
                    <li>Tokio's JoinSet provides efficient task-based parallelism</li>
                    <li>Message passing effectively decouples computation from I/O</li>
                    <li>Tournament selection eliminates global synchronization points</li>
                    <li>Chunking strategy balances parallelism and overhead</li>
                    <li>Benchmark mode enables accurate performance measurement</li>
                    <li>Thread safety guaranteed at compile time via Rust's type system</li>
                </ul>
                <p>These principles apply broadly to CPU-bound parallel workloads</p>
            </section>

            <section>
                <h2>Broader Implications</h2>
                <ul>
                    <li>Tokio excels beyond just I/O-bound workloads</li>
                    <li>Structured concurrency improves reliability for parallel algorithms</li>
                    <li>Message passing reduces synchronization overhead</li>
                    <li>Worker thread pools maximize CPU utilization</li>
                    <li>Compile-time thread safety eliminates entire class of bugs</li>
                </ul>
                <p>A free-threaded Python atop Tokio could bring these benefits to Python</p>
            </section>

            <section>
                <h2>Returning to the Initial Question</h2>
                <blockquote>
                    "Would Pythonistas benefit from free-threaded Python atop the Tokio runtime?"
                </blockquote>
                <p>My thoughts:</p>
                <ul>
                    <li>Potential for substantial performance gains</li>
                    <li>Challenges in API design and mental model</li>
                    <li>Integration complexity considerations</li>
                </ul>
            </section>

            <section>
                <h2>Sources and Credits</h2>
                <ul>
                    <li>@graingert and the anyio core contributing team</li>
                    <li>Rust for the Polyglot Programmer</li>
                    <li>Rust programming language book, rustlings, rust by example</li>
                    <li>Tokio glossary</li>
                    <li>Arden labs: Fearless Concurrency in Rust series</li>
                    <li>Piotr's performance benchmarking</li>
                    <li>David Hewitt Rust Nation UK talk</li>
                    <li>Evgenii Seliversov: Parallel Programming in Rust techniques</li>
                    <li>Rust async book</li>
                </ul>
            </section>

            <section>
                <h1>Thank You!</h1>
                <h3>Questions?</h3>
                <img src="images/iceberg.png" alt="Iceberg">
            </section>
        </div>
    </div>

    <!-- Ferris Runner -->
    <div class="ferris-runner">
        <img src="https://rustacean.net/assets/rustacean-flat-happy.svg" alt="Ferris">
    </div>

    <!-- Load all scripts at the end of body -->
    <!-- Core libraries -->
    <script src="https://cdn.jsdelivr.net/npm/highlight.js@11.8.0/highlight.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/highlight.js@11.8.0/languages/python.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/highlight.js@11.8.0/languages/rust.min.js"></script>
    
    <!-- Reveal.js and its plugins -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/plugin/markdown/markdown.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/plugin/math/math.js"></script>
    
    <!-- Mermaid plugin -->
    <script src="https://cdn.jsdelivr.net/npm/reveal.js-mermaid-plugin@11.4.1/plugin/mermaid/mermaid.min.js"></script>
    
    <!-- Custom script - load last -->
    <script src="script.js"></script>
</body>
</html>